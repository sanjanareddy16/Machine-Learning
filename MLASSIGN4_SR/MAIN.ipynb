{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379dea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISIONTREE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion=None, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.tree is None:\n",
    "            raise Exception(\"The tree has not been trained yet. Please fit the model first.\")\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        if depth >= self.max_depth or num_samples < self.min_samples_split or len(np.unique(y)) == 1:\n",
    "            return {'leaf': True, 'prediction': self._calculate_prediction(y)}\n",
    "\n",
    "        if self.criterion == 'gini':\n",
    "            best_split = self._get_best_split_gini(X, y)\n",
    "        elif self.criterion == 'entropy':\n",
    "            best_split = self._get_best_split_entropy(X, y)\n",
    "        elif self.criterion == 'misclassification':\n",
    "            best_split = self._get_best_split_misclassification(X, y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion. Supported values are 'gini', 'entropy', and 'misclassification'.\")\n",
    "\n",
    "        if best_split['left_X'].shape[0] < self.min_samples_leaf or best_split['right_X'].shape[0] < self.min_samples_leaf:\n",
    "            return {'leaf': True, 'prediction': self._calculate_prediction(y)}\n",
    "\n",
    "        left_tree = self._build_tree(best_split['left_X'], best_split['left_y'], depth + 1)\n",
    "        right_tree = self._build_tree(best_split['right_X'], best_split['right_y'], depth + 1)\n",
    "\n",
    "        return {'leaf': False, 'split_feature': best_split['split_feature'], 'split_value': best_split['split_value'],\n",
    "                'left_tree': left_tree, 'right_tree': right_tree}\n",
    "\n",
    "    def _get_best_split_gini(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                left_mask = X[:, feature] <= value\n",
    "        \n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                gini = (len(left_y) / num_samples) * self._calculate_gini(left_y) + \\\n",
    "                       (len(right_y) / num_samples) * self._calculate_gini(right_y)\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {'split_feature': feature, 'split_value': value,\n",
    "                                  'left_X': X[left_mask], 'left_y': left_y,\n",
    "                                  'right_X': X[right_mask], 'right_y': right_y}\n",
    "        return best_split\n",
    "\n",
    "    def _get_best_split_entropy(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_entropy = float('inf')\n",
    "        best_split = None\n",
    "\n",
    "        \n",
    "                \n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                left_mask = X[:, feature] <= value \n",
    "    \n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                entropy = (len(left_y) / num_samples) * self._calculate_entropy(left_y) + \\\n",
    "                          (len(right_y) / num_samples) * self._calculate_entropy(right_y)\n",
    "\n",
    "                if entropy < best_entropy:\n",
    "                    best_entropy = entropy\n",
    "                    best_split = {'split_feature': feature, 'split_value': value,\n",
    "                                  'left_X': X[left_mask], 'left_y': left_y,\n",
    "                                  'right_X': X[right_mask], 'right_y': right_y}\n",
    "        return best_split\n",
    "\n",
    "    def _get_best_split_misclassification(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_misclassification = float('inf')\n",
    "        best_split = None\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                left_mask = X[:, feature] <= value \n",
    "\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                misclassification = (len(left_y) / num_samples) * self._calculate_misclassification(left_y) + \\\n",
    "                                    (len(right_y) / num_samples) * self._calculate_misclassification(right_y)\n",
    "\n",
    "                if misclassification < best_misclassification:\n",
    "                    best_misclassification = misclassification\n",
    "                    best_split = {'split_feature': feature, 'split_value': value,\n",
    "                                  'left_X': X[left_mask], 'left_y': left_y,\n",
    "                                  'right_X': X[right_mask], 'right_y': right_y}\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_gini(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        p = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(p ** 2)\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "    \n",
    "        p = np.bincount(y) / len(y)\n",
    "        p = p[p > 0]  # Exclude zero probabilities to avoid divide by zero and log(0) issues\n",
    "        return -np.sum(p * np.log2(p))\n",
    "\n",
    "    def _calculate_misclassification(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        p = np.bincount(y) / len(y)\n",
    "        return 1 - np.max(p)\n",
    "\n",
    "    def _calculate_prediction(self, y):\n",
    "        # Calculate the class prediction based on the most frequent class in y\n",
    "        return np.argmax(np.bincount(y))\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node['leaf']:\n",
    "            return node['prediction']\n",
    "\n",
    "    \n",
    "        if x[node['split_feature']] <= node['split_value']:\n",
    "            return self._traverse_tree(x, node['left_tree'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node['right_tree'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129d2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, classifier, num_trees, min_features):\n",
    "        self.classifier = classifier\n",
    "        self.num_trees = num_trees\n",
    "        self.min_features = min_features\n",
    "        self.forest = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.forest = []\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "            # Sample with replacement\n",
    "            sampled_indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            X_sampled = X[sampled_indices]\n",
    "            y_sampled = y[sampled_indices]\n",
    "\n",
    "            # Select random subset of features\n",
    "            selected_features = np.random.choice(X.shape[1], self.min_features, replace=False)\n",
    "\n",
    "            # Use the provided DecisionTree instance directly\n",
    "            tree = self.classifier\n",
    "            tree.fit(X_sampled[:, selected_features], y_sampled)\n",
    "\n",
    "            self.forest.append({'tree': tree, 'selected_features': selected_features})\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.forest:\n",
    "            raise Exception(\"The random forest has not been trained yet. Please fit the model first.\")\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for tree_info in self.forest:\n",
    "            tree = tree_info['tree']\n",
    "            selected_features = tree_info['selected_features']\n",
    "            prediction = tree.predict(X[:, selected_features])\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array([np.argmax(np.bincount(p)) for p in np.array(predictions).T])\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9093c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOSTING\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, weak_learner, num_learners, learning_rate):\n",
    "        self.weak_learner = weak_learner\n",
    "        self.num_learners = num_learners\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learners = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        weights = np.ones(num_samples) / num_samples \n",
    "\n",
    "        for m in range(self.num_learners):\n",
    "            learner = self.weak_learner(criterion='gini', max_depth=4)  \n",
    "            learner.fit(X, y, sample_weight=weights)\n",
    "\n",
    "            predictions = learner.predict(X)\n",
    "            incorrect = predictions != y\n",
    "            error = np.sum(weights[incorrect])\n",
    "\n",
    "            if error == 0:\n",
    "                self.learners.append(learner)\n",
    "                self.alphas.append(1.0)\n",
    "                break\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            self.learners.append(learner)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            weights *= np.exp(-alpha * y * predictions)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.learners or not self.alphas:\n",
    "            raise Exception(\"error.\")\n",
    "\n",
    "        learner_predictions = np.array([learner.predict(X) for learner in self.learners])\n",
    "        weighted_sum = np.dot(self.alphas, learner_predictions)\n",
    "        return np.sign(weighted_sum).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f118857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8212290502793296\n",
      "Random Forest Accuracy: 0.6033519553072626\n",
      "Boosting Accuracy: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "#TITANIC DATASET\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "\n",
    "original_train = train.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n",
    "\n",
    "# Feature engineering steps taken from Sina and Anisotropic, with minor changes to avoid warnings\n",
    "full_data = [train, test]\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    # Next line has been improved to avoid warning\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] ;\n",
    "# Feature selection: remove variables no longer containing relevant information\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "\n",
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# DECISION TREE\n",
    "dt_model = DecisionTree(criterion='entropy', max_depth=3)\n",
    "dt_model.fit(X_train.values, y_train.values)\n",
    "dt_predictions = dt_model.predict(X_test.values)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "\n",
    "# RandomForest\n",
    "dt_classifier = DecisionTree(criterion='entropy', max_depth=4, min_samples_split=2, min_samples_leaf=1)\n",
    "random_forest = RandomForest(classifier=dt_classifier, num_trees=11, min_features=5)\n",
    "random_forest.fit(X_train.values, y_train.values)\n",
    "predictions = random_forest.predict(X_test.values)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy}\")\n",
    "\n",
    "# AdaBoost\n",
    "adaboost_classifier = AdaBoost(weak_learner=DecisionTree, num_learners=100, learning_rate=1.0)\n",
    "adaboost_classifier.fit(X_train.values, y_train.values)\n",
    "predictions = adaboost_classifier.predict(X_test.values)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Boosting Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f7dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
